{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a4704a",
   "metadata": {},
   "source": [
    "# ==========================\n",
    "# EXPLORATORY NOTEBOOK FOR HYBRID MUSIC CLUSTERING\n",
    "# ==========================\n",
    "\n",
    "# --------------------------\n",
    "# 1. IMPORT LIBRARIES\n",
    "# --------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------\n",
    "# 2. DATASET OVERVIEW\n",
    "# --------------------------\n",
    "\n",
    "# Example: Load lyrics CSV\n",
    "lyrics_csv_path = \"data/lyrics.csv\"  # change path accordingly\n",
    "df = pd.read_csv(lyrics_csv_path)\n",
    "print(\"First 5 rows of dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Check labels if available\n",
    "if \"genre\" in df.columns:\n",
    "    print(f\"Number of unique genres: {df['genre'].nunique()}\")\n",
    "if \"language\" in df.columns:\n",
    "    print(f\"Number of unique languages: {df['language'].nunique()}\")\n",
    "\n",
    "# --------------------------\n",
    "# 3. AUDIO FEATURE EXPLORATION\n",
    "# --------------------------\n",
    "\n",
    "# Load a sample audio file\n",
    "audio_file = \"data/audio/sample_song.wav\"  # replace with actual file\n",
    "y, sr = librosa.load(audio_file, sr=22050)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title(\"Waveform of sample_song.wav\")\n",
    "plt.show()\n",
    "\n",
    "# Compute MFCC\n",
    "mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfcc, sr=sr, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title(\"MFCC of sample_song.wav\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 4. LYRICS FEATURE EXPLORATION\n",
    "# --------------------------\n",
    "\n",
    "# Convert lyrics to TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_lyrics = tfidf.fit_transform(df['lyrics'].fillna(\"\"))\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_lyrics.shape}\")\n",
    "print(\"Top 10 words from TF-IDF vocabulary:\")\n",
    "print(list(tfidf.vocabulary_.keys())[:10])\n",
    "\n",
    "# --------------------------\n",
    "# 5. FEATURE REDUCTION: PCA\n",
    "# --------------------------\n",
    "\n",
    "# PCA on lyrics features\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_lyrics.toarray())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\n",
    "plt.title(\"PCA projection of lyrics features\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 6. LATENT FEATURE SANITY CHECK (OPTIONAL)\n",
    "# --------------------------\n",
    "# If you have trained a small VAE or AE on a subset\n",
    "# You can encode few samples to check separation\n",
    "\n",
    "# Example placeholder\n",
    "# Z_sample = vae.encode(X_sample)  # X_sample: small batch from data\n",
    "# Z_tsne = TSNE(n_components=2).fit_transform(Z_sample)\n",
    "# plt.scatter(Z_tsne[:, 0], Z_tsne[:, 1])\n",
    "# plt.title(\"t-SNE of VAE latent features (sample)\")\n",
    "# plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 7. QUICK CLUSTERING EXPLORATION\n",
    "# --------------------------\n",
    "\n",
    "# Example: KMeans on PCA features\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "plt.title(\"KMeans clusters on PCA projection\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 8. SUMMARY / OBSERVATIONS\n",
    "# --------------------------\n",
    "# Markdown cell content:\n",
    "\"\"\"\n",
    "# Summary of Exploratory Analysis\n",
    "\n",
    "1. Dataset contains {len(df)} samples with features from audio and lyrics.\n",
    "2. TF-IDF vectorization of lyrics produces {X_lyrics.shape[1]} features.\n",
    "3. PCA shows some separation in top components, but clusters are not perfectly distinct.\n",
    "4. Audio MFCC features show variability across songs; waveform visualizations confirm diverse durations and amplitudes.\n",
    "5. Preliminary KMeans clustering indicates potential grouping by genre/language, but final evaluation will require VAE latent features.\n",
    "6. Any missing lyrics or audio files are noted for preprocessing.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a0e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.dataset import load_hybrid_dataset\n",
    "from src.vae import VAE, Autoencoder\n",
    "from src.clustering import run_kmeans, evaluate_clustering\n",
    "from src.unsupervised_viz import plot_tsne\n",
    "\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "LATENT_DIR = os.path.join(RESULTS_DIR, 'latent_visualization')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(LATENT_DIR, exist_ok=True)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c130c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GTZAN Genre Collection...\n",
      "\n",
      "GTZAN dataset not found at ./music_data\\gtzan\\genres\n",
      "Please download from: http://marsyas.info/downloads/datasets.html\n",
      "Or provide pre-computed features file\n",
      "\n",
      "Generating sample data for demonstration...\n",
      "\n",
      "Generating 1000 sample music features with 43 dimensions...\n",
      "Sample data generated: 800 training, 200 test samples\n",
      "Feature dimension: 43, Classes: 10\n",
      "X shape: (1000, 75)\n",
      "y_language shape: (1000,) unique labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Load Hybrid Dataset (real data required)\n",
    "# Set these to your actual paths. No synthetic fallback is allowed.\n",
    "data_dir = \"./music_data\"  # e.g., path to GTZAN root containing gtzan/genres\n",
    "lyrics_csv = None  # e.g., path to lyrics CSV with columns [lyrics, language]\n",
    "\n",
    "if lyrics_csv is None:\n",
    "    raise FileNotFoundError(\"Provide a lyrics_csv path with lyrics and language columns.\")\n",
    "\n",
    "data = load_hybrid_dataset(\n",
    "    use_audio=True,\n",
    "    use_lyrics=True,\n",
    "    data_dir=data_dir,\n",
    "    lyrics_csv=lyrics_csv,\n",
    "    allow_fallback=False,\n",
    ")\n",
    "X = data['X_combined']\n",
    "y_lang = data.get('y_language', None)\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "if y_lang is not None:\n",
    "    print('y_language shape:', y_lang.shape, 'unique labels:', np.unique(y_lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb73f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USERAS\\Desktop\\715_Project\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 1.0068\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.9468  \n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8124  \n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6981 \n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6340 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>silhouette</th>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <th>davies_bouldin</th>\n",
       "      <th>ari</th>\n",
       "      <th>nmi</th>\n",
       "      <th>purity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.362467</td>\n",
       "      <td>423.09256</td>\n",
       "      <td>1.408809</td>\n",
       "      <td>0.383872</td>\n",
       "      <td>0.437522</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   silhouette  calinski_harabasz  davies_bouldin       ari       nmi  purity\n",
       "0    0.362467          423.09256        1.408809  0.383872  0.437522    0.81"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline: Autoencoder + KMeans\n",
    "latent_dim = 8\n",
    "ae = Autoencoder(input_dim=X.shape[1], latent_dim=latent_dim, hidden_dims=(256,128))\n",
    "_ = ae.fit_ae(X, batch_size=128, epochs=5, validation_data=None)\n",
    "Z_ae = ae.encode(X)\n",
    "\n",
    "k = int(np.max(y_lang)) + 1 if y_lang is not None else 8\n",
    "labels_ae = run_kmeans(Z_ae, n_clusters=k)\n",
    "metrics_ae = evaluate_clustering(Z_ae, labels_ae, y_true=y_lang)\n",
    "pd.DataFrame([metrics_ae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1872a133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/8\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3s/step - kl_loss: 2.2060 - reconstruction_loss: 78.7206 - total_loss: 80.9266"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No loss to compute. Provide a `loss` argument in `compile()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m vae \u001b[38;5;241m=\u001b[39m VAE(input_dim\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, hidden_dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m128\u001b[39m), beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m      3\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m Z \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mencode(X)\n\u001b[0;32m      7\u001b[0m labels_vae \u001b[38;5;241m=\u001b[39m run_kmeans(Z, n_clusters\u001b[38;5;241m=\u001b[39mk)\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\Desktop\\715_Project\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\Desktop\\715_Project\\.venv\\lib\\site-packages\\keras\\src\\trainers\\trainer.py:357\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    355\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_additional_loss(loss))\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo loss to compute. Provide a `loss` argument in `compile()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m     )\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    361\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: No loss to compute. Provide a `loss` argument in `compile()`."
     ]
    }
   ],
   "source": [
    "# VAE: Train and Cluster\n",
    "vae = VAE(input_dim=X.shape[1], latent_dim=latent_dim, hidden_dims=(256,128), beta=1.0)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.fit(X, epochs=5, batch_size=128, validation_split=0.1, verbose=1)\n",
    "Z = vae.encode(X)\n",
    "\n",
    "labels_vae = run_kmeans(Z, n_clusters=k)\n",
    "metrics_vae = evaluate_clustering(Z, labels_vae, y_true=y_lang)\n",
    "pd.DataFrame([metrics_vae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: t-SNE plots\n",
    "plot_tsne(Z_ae, labels_ae, title='AE+KMeans t-SNE', save_path=os.path.join(LATENT_DIR, 'ae_kmeans_tsne_notebook.png'))\n",
    "plot_tsne(Z, labels_vae, title='VAE+KMeans t-SNE', save_path=os.path.join(LATENT_DIR, 'vae_kmeans_tsne_notebook.png'))\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "figs = [os.path.join(LATENT_DIR, 'ae_kmeans_tsne_notebook.png'), os.path.join(LATENT_DIR, 'vae_kmeans_tsne_notebook.png')]\n",
    "for fp in figs:\n",
    "    if os.path.exists(fp):\n",
    "        display(Image.open(fp))\n",
    "    else:\n",
    "        print('Plot not found:', fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined metrics\n",
    "import json\n",
    "all_metrics = pd.DataFrame([\n",
    "    {**metrics_ae, 'method': 'AE+KMeans'},\n",
    "    {**metrics_vae, 'method': 'VAE+KMeans'}\n",
    "])\n",
    "metrics_path = os.path.join(RESULTS_DIR, 'clustering_metrics_notebook.csv')\n",
    "all_metrics.to_csv(metrics_path, index=False)\n",
    "print('Saved metrics to:', metrics_path)\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf1033",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
